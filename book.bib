
@article{Baker2009,
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	copyright = {2016 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/533452a},
	doi = {10.1038/533452a},
	abstract = {Survey sheds light on the ‘crisis’ rocking research.},
	language = {en},
	number = {7604},
	urldate = {2021-10-06},
	journal = {Nature},
	author = {Baker, Monya},
	month = may,
	year = {2016},
	pages = {452--454},
}

@misc{GenoFab2021,
	title = {Repeatability \& {Reproducibility}},
	url = {https://blog.genofab.com/repeatability-vs-reproducibility},
	abstract = {Repeatability vs Reproducibility – In science, reproducibility, not just repeatability, is key. Find out how to make research more reproducible.},
	language = {en},
	urldate = {2021-10-06},
}

@misc{Frey2021,
	title = {How and why to share scientific code},
	url = {https://towardsdatascience.com/how-and-why-to-share-scientific-code-64fbd385a67},
	abstract = {A simple guide to reproducible research without becoming a software engineer},
	language = {en},
	urldate = {2021-10-06},
	journal = {Medium},
	author = {Nathan C. Frey},
	month = apr,
	year = {2021},
}

@article{Sandve2013,
	title = {Ten {Simple} {Rules} for {Reproducible} {Computational} {Research}},
	volume = {9},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285},
	doi = {10.1371/journal.pcbi.1003285},
	language = {en},
	number = {10},
	urldate = {2021-10-06},
	journal = {PLOS Computational Biology},
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	month = oct,
	year = {2013},
	keywords = {Computer applications, Reproducibility, Archives, Computer and information sciences, Source code, Genome analysis, Habits, Replication studies},
	pages = {e1003285},
}

@article{Benureau2018,
	title = {Re-run, {Repeat}, {Reproduce}, {Reuse}, {Replicate}: {Transforming} {Code} into {Scientific} {Contributions}},
	volume = {11},
	issn = {1662-5196},
	shorttitle = {Re-run, {Repeat}, {Reproduce}, {Reuse}, {Replicate}},
	url = {https://www.frontiersin.org/article/10.3389/fninf.2017.00069},
	doi = {10.3389/fninf.2017.00069},
	abstract = {Scientific code is different from production software. Scientific code, by producing results that are then analyzed and interpreted, participates in the elaboration of scientific conclusions. This imposes specific constraints on the code that are often overlooked in practice. We articulate, with a small example, five characteristics that a scientific code in computational science should possess: re-runnable, repeatable, reproducible, reusable, and replicable. The code should be executable (re-runnable) and produce the same result more than once (repeatable); it should allow an investigator to reobtain the published results (reproducible) while being easy to use, understand and modify (reusable), and it should act as an available reference for any ambiguity in the algorithmic descriptions of the article (replicable).},
	urldate = {2021-10-06},
	journal = {Frontiers in Neuroinformatics},
	author = {Benureau, Fabien C. Y. and Rougier, Nicolas P.},
	year = {2018},
	pages = {69},
}

@article{BeaulieuJones2017,
	title = {Reproducibility of computational workflows is automated using continuous analysis},
	volume = {35},
	issn = {1087-0156},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6103790/},
	doi = {10.1038/nbt.3780},
	abstract = {Replication, validation and extension of experiments are crucial for scientific progress. Computational experiments are inherently scriptable and should be easy to reproduce. However, it remains difficult and time consuming to reproduce computational results because analyses are designed and run in a specific computing environment, which may be difficult or impossible to match from written instructions. We report a workflow named continuous analysis that can build reproducibility into computational analyses. Continuous analysis combines Docker, a container technology akin to virtual machines, with continuous integration, a software development technique, to automatically re-run a computational analysis whenever updates or improvements are made to source code or data. This allows results to be accurately reproduced without needing to contact the study authors. Continuous analysis allows reviewers, editors or readers to verify reproducibility without manually downloading and re-running any code and can provide an audit trail for analyses of data that cannot be shared.},
	number = {4},
	urldate = {2021-10-06},
	journal = {Nature biotechnology},
	author = {Beaulieu-Jones, Brett K. and Greene, Casey S.},
	month = apr,
	year = {2017},
	pmid = {28288103},
	pmcid = {PMC6103790},
	pages = {342--346},
}

@book{hester_happy_nodate,
	title = {Happy {Git} and {GitHub} for the {useR}},
	url = {https://happygitwithr.com/},
	abstract = {Using Git and GitHub with R, Rstudio, and R Markdown},
	urldate = {2021-10-06},
	author = {Jenny Bryan, Jim Hester},
}

@misc{SoftwareCarpentry2021,
	title = {Version {Control} with {Git}},
	url = {https://swcarpentry.github.io/git-novice/},
	urldate = {2021-10-06},
}

@misc{Vickery2021,
	title = {4 {Tools} for {Reproducible} {Jupyter} {Notebooks}},
	url = {https://towardsdatascience.com/4-tools-for-reproducible-jupyter-notebooks-d7423721bd04},
	abstract = {Jupyter notebooks have a somewhat poor reputation in the wider programming community. Joel Grus’ famous “I don’t like notebooks” talk, which he bravely gave at JupyterCon in 2018, covered many of the…},
	language = {en},
	urldate = {2021-10-06},
	journal = {Medium},
	author = {Vickery, Rebecca},
	month = may,
	year = {2021},
}

@techreport{Patil2016,
	title = {A statistical definition for reproducibility and replicability},
	copyright = {© 2016, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/066803v1},
	abstract = {Everyone agrees that reproducibility and replicability are fundamental characteristics of scientific studies. These topics are attracting increasing attention, scrutiny, and debate both in the popular press and the scientific literature. But there are no formal statistical definitions for these concepts, which leads to confusion since the same words are used for different concepts by different people in different fields. We provide formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press.},
	language = {en},
	urldate = {2021-10-06},
	author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
	month = jul,
	year = {2016},
	note = {Type: article},
	pages = {066803},
}

@article{Brito2020,
	title = {Recommendations to enhance rigor and reproducibility in biomedical research},
	volume = {9},
	issn = {2047-217X},
	url = {https://doi.org/10.1093/gigascience/giaa056},
	doi = {10.1093/gigascience/giaa056},
	abstract = {Biomedical research depends increasingly on computational tools, but mechanisms ensuring open data, open software, and reproducibility are variably enforced by academic institutions, funders, and publishers. Publications may present software for which source code or documentation are or become unavailable; this compromises the role of peer review in evaluating technical strength and scientific contribution. Incomplete ancillary information for an academic software package may bias or limit subsequent work. We provide 8 recommendations to improve reproducibility, transparency, and rigor in computational biology—precisely the values that should be emphasized in life science curricula. Our recommendations for improving software availability, usability, and archival stability aim to foster a sustainable data science ecosystem in life science research.},
	number = {6},
	urldate = {2021-10-06},
	journal = {GigaScience},
	author = {Brito, Jaqueline J and Li, Jun and Moore, Jason H and Greene, Casey S and Nogoy, Nicole A and Garmire, Lana X and Mangul, Serghei},
	month = jun,
	year = {2020},
}

@article{Hothorn2011,
	title = {Case studies in reproducibility},
	volume = {12},
	issn = {1467-5463},
	url = {https://doi.org/10.1093/bib/bbq084},
	doi = {10.1093/bib/bbq084},
	abstract = {Reproducible research is a concept of providing access to data and software along with published scientific findings. By means of some case studies from different disciplines, we will illustrate reasons why readers should be given the possibility to look at the data and software independently from the authors of the original publication. We report results of a survey comprising 100 papers recently published in Bioinformatics. The main finding is that authors of this journal share a culture of making data available. However, the number of papers where source code for simulation studies or analyzes is available is still rather limited.},
	number = {3},
	urldate = {2021-10-06},
	journal = {Briefings in Bioinformatics},
	author = {Hothorn, Torsten and Leisch, Friedrich},
	month = may,
	year = {2011},
	pages = {288--300},
}

@article{Ioannidis2009,
	title = {Repeatability of published microarray gene expression analyses},
	volume = {41},
	issn = {1546-1718},
	doi = {10.1038/ng.295},
	abstract = {Given the complexity of microarray-based gene expression studies, guidelines encourage transparent design and public data availability. Several journals require public data deposition and several public databases exist. However, not all data are publicly available, and even when available, it is unknown whether the published results are reproducible by independent scientists. Here we evaluated the replication of data analyses in 18 articles on microarray-based gene expression profiling published in Nature Genetics in 2005-2006. One table or figure from each article was independently evaluated by two teams of analysts. We reproduced two analyses in principle and six partially or with some discrepancies; ten could not be reproduced. The main reason for failure to reproduce was data unavailability, and discrepancies were mostly due to incomplete data annotation or specification of data processing and analysis. Repeatability of published microarray studies is apparently limited. More strict publication rules enforcing public data availability and explicit description of data processing and analysis should be considered.},
	language = {eng},
	number = {2},
	journal = {Nature Genetics},
	author = {Ioannidis, John P. A. and Allison, David B. and Ball, Catherine A. and Coulibaly, Issa and Cui, Xiangqin and Culhane, Aedín C. and Falchi, Mario and Furlanello, Cesare and Game, Laurence and Jurman, Giuseppe and Mangion, Jon and Mehta, Tapan and Nitzberg, Michael and Page, Grier P. and Petretto, Enrico and van Noort, Vera},
	month = feb,
	year = {2009},
	pmid = {19174838},
	keywords = {Animals, Data Interpretation, Statistical, Databases, Genetic, Gene Expression Profiling, Genome-Wide Association Study, Humans, Oligonucleotide Array Sequence Analysis, Peer Review, Research, Publications, Reproducibility of Results},
	pages = {149--155},
}

@article{Baker2016,
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	copyright = {2016 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/533452a},
	doi = {10.1038/533452a},
	abstract = {Survey sheds light on the ‘crisis’ rocking research.},
	language = {en},
	number = {7604},
	urldate = {2021-10-06},
	journal = {Nature},
	author = {Baker, Monya},
	month = may,
	year = {2016},
	pages = {452--454},
}

@Manual{rmarkdown2021,
  title = {rmarkdown: Dynamic Documents for R},
  author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},
  year = {2021},
  note = {R package version 2.10},
  url = {https://github.com/rstudio/rmarkdown},
}

@Book{Xie2018,
  title = {R Markdown: The Definitive Guide},
  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2018},
  note = {ISBN 9781138359338},
  url = {https://bookdown.org/yihui/rmarkdown},
}

@Book{Xie2020,
  title = {R Markdown Cookbook},
  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2020},
  note = {ISBN 9780367563837},
  url = {https://bookdown.org/yihui/rmarkdown-cookbook},
}
