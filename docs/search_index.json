[["index.html", "Reproducibility in Cancer Informatics About this Course", " Reproducibility in Cancer Informatics October, 2021 About this Course This course is part of a series of courses for the Informatics Technology for Cancer Research (ITCR) called the Informatics Technology for Cancer Research Education Resource. This material was created by the ITCR Training Network (ITN) which is a collaborative effort of researchers around the United States to support cancer informatics and data science training through resources, technology, and events. This initiative is funded by the following grant: National Cancer Institute (NCI) UE5 CA254170. Our courses feature tools developed by ITCR Investigators and make it easier for principal investigators, scientists, and analysts to integrate cancer informatics into their workflows. Please see our website at www.itcrtraining.org for more information. "],["introduction.html", "Chapter 1 Introduction 1.1 Motivation 1.2 Target Audience 1.3 Curriculum", " Chapter 1 Introduction 1.1 Motivation Cancer datasets are plentiful, complicated, and hold untold amounts of information regarding cancer biology. Cancer researchers are working to apply their expertise to the analysis of these vast amounts of data but training opportunities to properly equip them in these efforts can be sparse. This includes training in reproducible data analysis methods. Data analyses are also are generally not reproducible without direct contact with the original researchers and a substantial amount of time and effort (BeaulieuJones2017 and Greene 2017). Reproducibility in cancer informatics (as with other fields) is still not monitored or incentivized despite that it is fundamental to the scientific method. Despite the lack of incentive, many researchers strive for reproducibility in their own work but often lack the skills or training to do so effectively. Equipping researchers with the skills to create reproducible data analyses increases the efficiency of everyone involved. Reproducible analyses are more likely to understood, applied, and replicated by others. This helps expedite the scientific process by helping researchers avoid false positive dead ends. Open source clarity in reproducible methods also saves researchers’ time so they don’t have to reinvent the proverbial wheel for methods that everyone in the field is already performing. This course introduces the concepts of reproducibility and replicability in the context of cancer informatics. It uses hands-on exercises to demonstrate in practical terms how to increase the reproducibility of data analyses. The course also introduces tools relevant to reproducibility including analysis notebooks, Docker images, git and GitHub. 1.2 Target Audience The course is intended for students in the biomedical sciences and researchers who use informatics tools in their research. 1.3 Curriculum The course includes a hands-on exercises for how to apply reproducible code concepts to their code. Individuals who take this course are encouraged to complete these activities as they follow along with the course material to help increase the reproducibility of their analyses. References "],["defining-reproducibility.html", "Chapter 2 Defining reproducibility 2.1 Learning Objectives 2.2 What is reproducibility 2.3 Reproducibility in daily life 2.4 Reproducibility is worth the effort! 2.5 Reproducibility exists on a continuum!", " Chapter 2 Defining reproducibility 2.1 Learning Objectives 2.2 What is reproducibility There’s been a lot of discussion about what is included in the term reproducibility and there is some discrepancy between fields. For the purposes of informatics and data analysis, a reproducible analysis is one that can be re-run by a different researcher and the same result and conclusion is found. Reproducibility is related to repeatability and replicability but it is worth taking the time parse out these terms. Perhaps you are like Ruby and have just found an interesting pattern through your data analysis! This has probably been the result of many months or years on your project and its worth celebrating! But before you get too excited, Ruby should ask herself whether she is able to re-run her own analysis and get the same results again. This is known as repeatability. Given that Ruby’s analysis is repeatable; she may feel confident now to share her preliminary results with her colleague, Avi the Associate. Whether or not someone else will be able to take Ruby’s code and data, re-run the analysis and obtain the same results is known as reproducibility. If Ruby’s results are able to be reproduced by Avi, now Avi may collect new data and use Ruby’s same analysis methods to analyze his data. Whether or not Avi’s new data and results concur with Ruby’s study’s original inferences is known as replicability. You may realize that these levels of research build on each other (like science is supposed to do). In this way, we can think of these in a hierarchy. Skipping any of these levels of research applicability can lead to unreliable results and conclusions. Science progresses when data and hypotheses are put through these levels thoroughly and sequentially. If results are not repeatable, they won’t be reproducible or replicable. Ideally all analyses and results would be reproducible without too much time effort spent; this would aid in the efficiency of research getting to the next stages and questions. But unfortunately, in practice, reproducibility is not as commonplace as we would hope. Institutions and reward systems generally do not prioritize or even measure reproducibility standards in research and training opportunities for reproducible techniques can be scarce. Reproducible research can often feel like uphill battle that is made steeper by lack of training opportunities. In this course, we hope to equip your research with the tools you need to enhance the reproducibility of your analyses so this uphill battle is less steep. 2.3 Reproducibility in daily life What does reproducibility in mean in the daily life of a researcher? Let’s say Ruby’s results are repeatable in her own hands and she excitedly tells her associate, Avi about her preliminary findings. Avi is very excited about these results as well as Ruby’s methods! Avi is also interested in Ruby’s analysis methods and results. So Ruby sends Avi the code and data she used to obtain the results. Now, whether or not Avi is able to obtain the same exact results with this same data and same analysis code will indicate if Ruby’s analysis is reproducible. Ruby may have spent a lot of time on her code and getting it to work on her computer, but whether it will successfully work on Avi’s computer is another story. Often when researchers share their analysis code it leads to a substantial amount of effort on the part of the researcher who has received the code to get it working and this often cannot be done successfully without help from the original code author (BeaulieuJones2017 and Greene 2017). Avi is encountering errors because Ruby’s code was written with Ruby’s computer and local set up in mind and she didn’t know how to make it more generally applicable. Avi is spending a lot of time just trying to re-run Ruby’s same analysis on her same data; he has yet to be able to try the code on any additional data (which will likely bring up even more errors). Avi is still struggling to work with Ruby’s code and is confused about the goals and approaches the code is taking. After struggling with Avi’s code for an untold amount of time, Avi may decide its time to email Ruby to get some clarity. Now both Avi and Ruby are confused about why this analysis isn’t nicely re-running for Avi. Their attempts to communicate about the code through email haven’t helped them clarify anything. Multiple versions of the code may have been sent back and forth between them and now things are taking a lot more time than either of them expected. Perhaps at some point Avi is able to successfully run Ruby’s code on Ruby’s same data. Just because Avi didn’t get any errors doesn’t mean that the code ran exactly the same as it did for Ruby. Lack of errors also doesn’t mean that either Ruby or Avi’s runs of the code ran with high accuracy or that the results can be trusted. Even a small difference in decimal point may indicate a more fundamental difference in how the analysis was performed and this could be due to differences in software versions, settings, or any number of items in their computing environments. 2.4 Reproducibility is worth the effort! Perhaps you’ve found yourself in a situation like Ruby and Avi; struggling to re-run code that you thought for sure was working a minute ago. In the upcoming chapters, we will discuss how to bolster your projects’ reproducibility. As you apply these reproducible techniques to your research may feel like it is taking more time to reach endpoints, but keep in mind that reproducible analyses and projects have higher upfront costs but these will absolutely pay off in the long term. Reproducibility in your analyses is not only a time saver for yourself, but also your colleagues, your field, and your future self! You might not change a single character in your code but then return to it in a a few days/months/years and find that it no longer runs! Reproducible code stands the test of time longer, making future you glad you spent the time to work on it. It’s said that your closest collaborator is you from 6 months ago but you don’t reply to email. Broman (n.d.) Many a data scientist has referred to their frustration with their past selves: Dear past-Hadley: PLEASE COMMENT YOUR CODE BETTER. Love present-Hadley — Hadley Wickham (@hadleywickham) April 7, 2016 The more you comment your code, and make it clear and readable, your future self will thank you. Reproducible code also saves your colleagues time! The more reproducible your code is, the less time all of your collaborators will need to spend troubleshooting it. The more people who use your code and need to try to fix it, the more time is wasted. This can add up to a lot of wasted researcher time and effort. But, reproducible code saves everyone exponential amounts of time and effort! It will also motivate individuals to use and cite your code and analyses in the future! 2.5 Reproducibility exists on a continuum! Incremental work on your analyses is good! You do not need to make your analyses perfect on the first try or even in a particular time frame. The first step in creating an analysis is to get it to work once! But the work does not end there. Furthermore, no analysis is or will ever be perfect in that it will not be reproducible in every single context throughout time. But somewhere toward the right of this continuum is what we will aim for. References "],["how-to-use-this-course.html", "Chapter 3 How to use this course 3.1 Getting comfortable with command line 3.2 Software requirements 3.3 Downloading the chapter files 3.4 The Reproducibility checklist", " Chapter 3 How to use this course To get the most out of this course, you should complete the exercises that come along with each chapter. The exercises in each chapter have a set of example project files for you to practice applying the reproducibility concepts we discuss. We will walk through step-by-step how to transform these example project files into a nicely reproducible project that creates a nice heatmap! These files come in Python or R options depending on your preference your resulting heatmap should resemble one of these: This is the R version of the heatmap: This is the Python version of the heatmap: We encourage you to apply the techniques and tools we use in the exercises to your own current and former projects that may exist on your computer! 3.1 Getting comfortable with command line We will have the download commands typed out for you that you will copy and paste to your command line. If you are not comfortable with command line, or don’t know how to find it, read this article. 3.2 Software requirements For this course there are two software requirements you will need installed on your computer: git and Docker. Both of these items are useful tools for creating reproducible analyses and will be helpful for you far beyond this course. We hope that our introduction to these tools will help you incorporate them into your daily work and development process. Before we install them, let’s explain what git and Docker are and why they aid in reproducibility. 3.2.1 Why Docker For this course, we will be working from a Docker image. Perhaps you’ve tried to run code you’ve obtained from a collaborator, or even code you developed a few months ago and obtained errors like what Avi is experiencing. Code often has software requirements in order for it to run (often called dependencies), and those dependencies have other dependencies they require and so on. Particular versions of software packages require particular versions of other software packages and this very easily becomes a tangled web of software that can feel impossible to untangle, such that it is colloquially known as “dependency hell”. In reality the computing environments are not as simple as shown with Ruby and Avi’s graphs above, they may become as nasty as this R inter-dependency graph shown here: Not only is this a horrible headache to set up and detangle on your local computer, the software versions you use also influence reproducibility of your results! Even if Avi is able to run the analysis, he may not get the same result! This concept was illustrated by BeaulieuJones2017 and Greene (2017). Using an example of identifying lists of differentially expressed genes, BeaulieuJones2017 and Greene (2017) illustrate on the left (labeled A) how different software versions can influence the results of an analysis – rendering the analysis. But on the right (labeled B) they illustrate how using a container can make results reproducible, regardless of an individual’s operating system and local set up. What does container-ized approach mean? If Ruby wants to ensure that Avi has the same set up for reproducing her results, she could mail her own physical computer to Avi, but clearly that’s not practical because she has other work to do. So instead, she can set up and run her analysis on a Docker container. With Docker she is able to send that container which contains the exact computing environment she used for her analysis, to Avi. Now Avi can run the analysis knowing that he is using the exact same set up that Ruby did. He can avoid dependency hell and also has a better chance of reproducing the exact results Ruby did. Using a container is a useful tool for reproducibility! 3.2.1.1 Getting started with Docker We will introduce you to how to use a Docker container for this course, but first you will need to install Docker on your computer. After you are able to install Docker and git on your computer there will be no other software installations you will need to deal with for this course! Go here and install Docker. Choose your operating system and follow along with their instructions. You ideally should follow the prompts to create a Dockerhub account as well. Dockerhub is the online service that can store docker images. 3.2.2 Why git git is a version control system that is a great tool for creating reproducible analyses. What is version control? Ruby here is experiencing a lack of version control and could probably benefit from using git. All of us at one point or another have created different versions of a file or document, but for analysis projects this can easily get out of hand if you don’t have a system in place. That’s where git comes in handy. Version control systems like git allow you to: Generate backups Test and experiment Keep history and track changes Collaborate and contribute Excerpts from article by (Harvie2019?). Version control is a necessary tool for creating reproducible analyses. and to share the code easily, we often use GitHub, which is an online hosting service for git controlled files. We’ve chosen to introduce you to git and GitHub because they are the most commonly used but the git terminology can take a bit getting used to, so we’ll start by introducing you to some terms that are often thrown around in the world of git. 3.2.3 Installing git To start, follow the installation instructions here according to your operating system. 3.2.4 “git”ting started Further reading for getting familiar with git and GitHub: A quick primer to version control 3.3 Downloading the chapter files At the exercise portion of each chapter, we will prompt you to follow either the R or Python exercise. There is no need to do both, they are largely the same. You will download either the R or Python project files and edit the files along with the course according to the prompts. These example project files will be zipped up and you will need to unzip them before you can complete the exercise. On most operating systems, you will be able to double click your chapter zip file to unzip, but for Windows you may have to follow these instructions). Please let us know with the feedback form if you encounter any problems along the way while completing the exercises. 3.4 The Reproducibility checklist As you are proceeding through this course, you may want to apply some of these tools and techniques to your own analyses. That is highly encouraged! Remember that analyses are not written perfectly on the first try (or the 2nd, or the nth, or ever!) but the more you are able to apply these principles to your analyses, the more future you (and perhaps your collaborators) will thank you for it! We’ve created this checklist as a summarized guide of the course to help you of this course to help you evaluate in what areas your analyses could use a boost of reproducibility. Download the checklist This checklist is written in markdown format – a short hand for html that can be rendered nicely like a webpage. You may want to store it with your analyses and perhaps on a GitHub repository (GitHub repositories are something we will walk through in the advanced course). We hope you find this course useful and informative (and maybe even fun!). References "],["organizing-your-project.html", "Chapter 4 Organizing your project 4.1 Learning Objectives 4.2 Organizational strategies 4.3 Readings about organizational strategies for data science projects: 4.4 Exercise: Organize your project!", " Chapter 4 Organizing your project 4.1 Learning Objectives Keeping your files organized is a skill that has a high long-term pay off. As you are in the thick of an analysis, you may underestimate how many files and terms you have floating around. But a short time later, you may return to your files and realize your organization was not as clear as you hoped. Tayo (2019) discusses four particular reasons why it is important to organize your project: Organization increases productivity. If a project is well organized, with everything placed in one directory, it makes it easier to avoid wasting time searching for project files such as datasets, codes, output files, and so on. A well-organized project helps you to keep and maintain a record of your ongoing and completed data science projects. Completed data science projects could be used for building future models. If you have to solve a similar problem in the future, you can use the same code with slight modifications. A well-organized project can easily be understood by other data science professionals when shared on platforms such as Github. Organization is yet another aspect of reproducibility that saves you and your colleagues time! 4.2 Organizational strategies There’s a lot of ways to keep your files organized, and there’s not a “one size fits all” organizational solution (Shapiro et al. 2021). In this chapter, we will discuss some generalities but as far as specifics we will point you to others’ who have written about works for them and advise that you use them as inspiration to figure out a strategy that works for you and your team. The most important aspects of your project organization scheme is that it: Is project-oriented (Bryan 2017). Follows consistent patterns. Is easy for you and others to find the files you need quickly. Minimizes the likelihood for errors (like writing over files accidentally). Is something maintainable! (Shapiro et al. 2021) 4.2.1 Tips for organizing your project: Getting more specific, here’s some ideas of how to organize your project: Make file names informative to those who don’t have knowledge of the project but avoid using spaces, quotes, or unusual characters in your filenames and folders – these only serve to make reading in files a nightmare in some programs. Number scripts in the order that they are run. Keep like-files together in their own directory: results tables with other results tables, etc. Including most importantly keeping raw data separate from processed data or other results! Put source scripts and functions in their own directory. Things that should never need to be called directly by yourself or anyone else. Put output in its own directories like results and plots. Have a central document (like a README) that describes the basic information about the analysis and how to re-run it. Make it easy on yourself, dates aren’t necessary. The computer keeps track of those. Make a central script that re-runs everything – including the creation of the folders! (more on this in a later chapter) Let’s see what these principles might look like put into practice. 4.2.1.1 Example organizational scheme Here’s an example of what this might look like: project-name/ ├── run_analysis.sh # Central script that runs everything again ├── 00-download-data.sh # The script that needs to be run first and is called by run_analysis.sh ├── 01-make-heatmap.Rmd # The script that needs to be run second and is also called by run_analysis.sh ├── README.md # The document that has the information that will orient someone to this project ├── plots/ # A folder of plots and resulting images │ └── project-name-heatmap.png ├── results/ # A folder results │ └── top_gene_results.tsv ├── raw-data/ # data files as they first arrive and **nothing** has been done to them yet. │ ├── project-name-raw.tsv │ └── project-name-metadata.tsv ├── processed-data/ # data that has been modified from the raw in some way. │ ├── project-name-quantile-normalized.tsv └── util/ # A folder of utilities that never needs to be called or touched directly unless troubleshooting something ├── plotting-functions.R └── data-wrangling-functions.R 4.3 Readings about organizational strategies for data science projects: But you don’t have to take my organizational strategy, there are lots of ideas out there. You can read through some of these articles to think about what kind of organizational strategy might work for you and your team: Jenny Bryan’s organizational strategy (Bryan and Hester, n.d.). Jenny Bryan on Project-oriented workflows(Bryan 2017). Data Carpentry mini-course about organizing projects (“Project Organization and Management for Genomics” n.d.). Andrew Severin’s strategy for organization (Severin n.d.). A BioStars thread where many individuals share their own organizational strategies (“How Do You Manage Your Files &amp; Directories for Your Projects?” n.d.). Data Carpentry course chapter about getting organized (“Introduction to the Command Line for Genomics” n.d.). 4.4 Exercise: Organize your project! 4.4.0.1 Get the example project files for this chapter For this chapter, we will organize our example project files! Now choose which version of the example you would like to work with and follow the instructions. Python version of the exercise To get the Python project example files, click this link. Now double click your chapter zip file to unzip. For Windows you may have to follow these instructions). Now let’s take a look at the files inside these projects. These are purposely not organized because for this exercise we will organize them! We can use the ls command to list the files in this folder. ## aggregated_metadata.json ## aml_heatmap.png ## analysis_OLD.py ## analysis.py ## dataset.zip ## LICENSE.TXT ## metadata_SRP070849.tsv ## SRP070849.tsv Organize these files! For now we will organize these files by hand, but in the upcoming chapters we will make it so our analysis places these items in the correct directories (and creates the directories if they do not exist!). 4.4.1 The task: Create a plots, results, and data folder and organize the files into their respective folders. Note that aggregated_metadata.json and LICENSE.TXT also belong in the `data folder. Delete any files that say “OLD”. Keeping multiple versions of your scripts around is a recipe for mistakes and confusion. In the advanced course we will discuss how to use version control to help you track this more elegantly. R version of the exercise To get the R project examples files click this link. Or you can use these commands in command line. Now double click your chapter zip file to unzip. For Windows you may have to follow these instructions). Now let’s take a look at the files inside these projects. These are purposely not organized because for this exercise we will organize them! We can use the ls command to list the files in this folder. ## aggregated_metadata.json ## dataset.zip ## heatmap_up_to_date_OLD.R ## heatmap_up_to_date.R ## LICENSE.TXT ## metadata_SRP070849.tsv ## SRP070849.tsv Organize these files! For now we will organize these files by hand, but in the upcoming chapters we will make it so our analysis places these items in the correct directories (and creates the directories if they do not exist!). 4.4.2 The task: Create a plots, results, and data folder and organize the files into their respective folders. Note that aggregated_metadata.json and LICENSE.TXT also belong in the data folder. Delete any files that say “OLD”. Keeping multiple versions of your scripts around is a recipe for mistakes and confusion. In the advanced course we will discuss how to use version control to help you track this more elegantly. After your files are organized, you are ready to move on to the next chapter and create a notebook! References "],["using-notebooks.html", "Chapter 5 Using Notebooks 5.1 Learning Objectives 5.2 Exercise: Convert code into a notebook!", " Chapter 5 Using Notebooks 5.1 Learning Objectives Notebooks are a handy way to have the code, output, and scientist’s thought process all documented in one place that is easy for others to read and follow. The notebook environment is incredibly useful for reproducible data science for a variety of reasons: 5.1.0.1 Reason 1: Notebooks allow for tracking data exploration and encourage the scientist to narrate their thought process: Each executed code cell is an attempt by the researcher to achieve something and to tease out some insight from the data set. The result is displayed immediately below the code commands, and the researcher can pause and think about the outcome. As code cells can be executed in any order, modified and re-executed as desired, deleted and copied, the notebook is a convenient environment to iteratively explore a complex problem. (Fangohr 2021) 5.1.0.2 Reason 2: Notebooks allow for easy sharing of results: Notebooks can be converted to html and pdf, and then shared as static read-only documents. This is useful to communicate and share a study with colleagues or managers. By adding sufficient explanation, the main story can be understood by the reader, even if they wouldn’t be able to write the code that is embedded in the document. (Fangohr 2021) 5.1.0.3 Reason 3: Notebooks can be re-ran as a script or developed interactively: A common pattern in science is that a computational recipe is iteratively developed in a notebook. Once this has been found and should be applied to further data sets (or other points in some parameter space), the notebook can be executed like a script, for example by submitting these scripts as batch jobs. (Fangohr 2021) This can also be handy especially if you use automation to enhance the reproducibility of your analyses (something we will talk about in the advanced part of this course). Because all of these reasons, we encourage the use of computational notebooks as a means of enhancing reproducibility. (This course itself is also written with the use of notebooks!) 5.2 Exercise: Convert code into a notebook! For this chapter, we will create notebooks from our example files code. If you have not followed these instructions to set up your environment for using the example files from this course, you will need to do so before starting on this exercise. Now choose which version of the example you would like to work with and follow the instructions. Python version of the exercise 5.2.1 Set up the example files To get the Python project example files, click this link. Now double click your chapter zip file to unzip. For Windows you may have to follow these instructions). Now let’s take a look at the files inside these projects. We can use the ls command to list the files in this folder. ## make-heatmap.py ## plots ## raw-data 5.2.2 The task 5.2.2.1 Set up your environment R version of the exercise 5.2.3 Set up the example files To get the R project examples files click this link. Or you can use these commands in command line. Now double click your chapter zip file to unzip. For Windows you may have to follow these instructions). Now let’s take a look at the files inside these projects. We can use the ls command to list the files in this folder. ## make_heatmap.R ## raw-data 5.2.4 The task Now that you’ve created your notebook, you are ready to start polishing that code! References "],["organizing-your-project-1.html", "Chapter 6 Organizing your project 6.1 Learning Objectives 6.2 Readings about reproducible, durable code 6.3 Exercise: Make code more durable!", " Chapter 6 Organizing your project 6.1 Learning Objectives 6.2 Readings about reproducible, durable code 6.3 Exercise: Make code more durable! 6.3.0.1 Get the example project files for this chapter "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Pedagogy Lead Content Instructor(s) Candace Savonen Lecturer(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved Delivered the course in some way - video or audio Content Author(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved If any other authors besides lead instructor Content Contributor(s) (include section name/link in parentheses) - make new line if more than one section involved Wrote less than a chapter Content Editor(s)/Reviewer(s) Checked your content Content Director(s) Helped guide the content direction Content Consultants (include chapter name/link in parentheses or word “General”) - make new line if more than one chapter involved Gave high level advice on content Acknowledgments Gave small assistance to content but not to the level of consulting Production Content Publisher Ira Gooding Content Publishing Reviewer(s) Reviewed overall content and aesthetics on publishing platform Technical Course Publishing Engineer(s) Helped with the code for the technical aspects related to the specific course generation Template Publishing Engineers Candace Savonen, Carrie Wright Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (Leanbuild) John Muschelli, Candace Savonen, Carrie Wright Art and Design Illustrator(s) Created graphics for the course Figure Artist(s) Created figures/plots for course Videographer(s) Filmed videos Videography Editor(s) Edited film Audiographer(s) Recorded audio Audiography Editor(s) Edited audio recordings Funding Funder(s) Institution/individual who funded course including grant number Funding Staff Staff members who help with funding   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 20.04.2 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2021-10-21 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] RSPM (R 4.0.3) ## backports 1.1.10 2020-09-15 [1] RSPM (R 4.0.2) ## bookdown 0.24 2021-09-29 [1] Github (rstudio/bookdown@88bc4ea) ## callr 3.4.4 2020-09-07 [1] RSPM (R 4.0.2) ## cli 2.0.2 2020-02-28 [1] RSPM (R 4.0.0) ## crayon 1.3.4 2017-09-16 [1] RSPM (R 4.0.0) ## desc 1.2.0 2018-05-01 [1] RSPM (R 4.0.3) ## devtools 2.3.2 2020-09-18 [1] RSPM (R 4.0.3) ## digest 0.6.25 2020-02-23 [1] RSPM (R 4.0.0) ## ellipsis 0.3.1 2020-05-15 [1] RSPM (R 4.0.3) ## evaluate 0.14 2019-05-28 [1] RSPM (R 4.0.3) ## fansi 0.4.1 2020-01-08 [1] RSPM (R 4.0.0) ## fs 1.5.0 2020-07-31 [1] RSPM (R 4.0.3) ## glue 1.4.2 2020-08-27 [1] RSPM (R 4.0.3) ## htmltools 0.5.0 2020-06-16 [1] RSPM (R 4.0.1) ## jquerylib 0.1.1 2020-04-30 [1] RSPM (R 4.0.0) ## knitr 1.33 2021-09-29 [1] Github (yihui/knitr@a1052d1) ## lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.0.2) ## magrittr 1.5 2014-11-22 [1] RSPM (R 4.0.0) ## memoise 1.1.0 2017-04-21 [1] RSPM (R 4.0.0) ## pkgbuild 1.1.0 2020-07-13 [1] RSPM (R 4.0.2) ## pkgload 1.1.0 2020-05-29 [1] RSPM (R 4.0.3) ## prettyunits 1.1.1 2020-01-24 [1] RSPM (R 4.0.3) ## processx 3.4.4 2020-09-03 [1] RSPM (R 4.0.2) ## ps 1.3.4 2020-08-11 [1] RSPM (R 4.0.2) ## purrr 0.3.4 2020-04-17 [1] RSPM (R 4.0.3) ## R6 2.4.1 2019-11-12 [1] RSPM (R 4.0.0) ## remotes 2.2.0 2020-07-21 [1] RSPM (R 4.0.3) ## rlang 0.4.10 2021-09-29 [1] Github (r-lib/rlang@f0c9be5) ## rmarkdown 2.10 2021-09-29 [1] Github (rstudio/rmarkdown@02d3c25) ## rprojroot 1.3-2 2018-01-03 [1] RSPM (R 4.0.0) ## sessioninfo 1.1.1 2018-11-05 [1] RSPM (R 4.0.3) ## stringi 1.5.3 2020-09-09 [1] RSPM (R 4.0.3) ## stringr 1.4.0 2019-02-10 [1] RSPM (R 4.0.3) ## testthat 3.0.1 2021-09-29 [1] Github (R-lib/testthat@e99155a) ## usethis 2.0.1.9000 2021-09-29 [1] Github (r-lib/usethis@3398055) ## withr 2.3.0 2020-09-22 [1] RSPM (R 4.0.2) ## xfun 0.26 2021-09-29 [1] Github (yihui/xfun@74c2a66) ## yaml 2.2.1 2020-02-01 [1] RSPM (R 4.0.3) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
